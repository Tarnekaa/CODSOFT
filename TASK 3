# ğŸ’ Step 1: Import our magic toolbox
import pandas as pd  # Think of this as Excel but with superpowers
from sklearn.model_selection import train_test_split  # Splits data like sharing snacks with friends
from sklearn.preprocessing import StandardScaler  # Makes features fair (no one gets an unfair advantage!)
from sklearn.metrics import classification_report  # Report card for our models
from sklearn.linear_model import LogisticRegression  # Old-school but reliable
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Tree armies ğŸŒ² + rocket boosters ğŸš€

# ğŸ“‚ Step 2: Load the dataset
df = pd.read_csv("your_dataset.csv")  # Replace with your actual file name
print("ğŸ“š Data loaded successfully!")

# ğŸ‘€ Step 3: Quick look at the data
print("ğŸ” First few rows of data:")
print(df.head())

# ğŸ§¹ Step 4: Clean the data
df = df.drop(columns=["customerID"])  # IDs are like roll numbers â€” not useful for predictions
df = df.dropna()  # Bye-bye missing values ğŸ‘‹

# ğŸ¨ Step 5: Convert words into numbers
df = pd.get_dummies(df, drop_first=True)  # One-hot encoding = turning categories into 0s and 1s

# ğŸ¯ Step 6: Separate features (X) and target (y)
X = df.drop("Churn", axis=1)  # Features = student behavior
y = df["Churn"]  # Target = did they drop out (churn) or stay?

# âœ‚ï¸ Step 7: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("ğŸ“Š Training and testing sets ready!")

# ğŸ“ Step 8: Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ğŸ¤– Step 9: Train and test different models

# ğŸ§® Logistic Regression â€” the "basic calculator" of ML
log_model = LogisticRegression()
log_model.fit(X_train, y_train)
log_preds = log_model.predict(X_test)
print("ğŸ“Š Logistic Regression Results:")
print(classification_report(y_test, log_preds))

# ğŸŒ² Random Forest â€” imagine a whole forest of decision trees voting
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
print("ğŸŒ² Random Forest Results:")
print(classification_report(y_test, rf_preds))

# ğŸš€ Gradient Boosting â€” trees that learn from each other step by step
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)
gb_preds = gb_model.predict(X_test)
print("ğŸš€ Gradient Boosting Results:")
print(classification_report(y_test, gb_preds))
